---
title: 서비스 장애의 원인을 찾아가기 위한 여러가지 시도
date: 2022-05-01
tags:
- Service Failure
- Synthetic Monitoring
---

현재 조직에서 하나의 시스템을 여러 고객이 보유한 환경에서 이용할 수 있도록 전환하다보니 자체적으로 운영 시 정상적으로 동작했던 시스템에서 여러가지 서비스 장애가 발생하였고 이에 대한 원인을 파악하기 힘들어서 스트레스까지 받는 상황이 벌어졌습니다. 여러가지 서비스 장애의 원인을 찾아가기 위해 시도한 여러가지 방법에 대해서 기록하려고 합니다. 조직에서 만드는 서비스가 규모가 작은 시스템일지라도 수 많은 고객의 요구사항으로 인해 많은 기능이 수정되고 추가됨에 따라서 사이드 이펙트로 인한 장애가 지속적으로 발생하였고 심지어는 이유를 쉽게 파악할 수 없는 인프라로 인한 문제 또는 여러가지 기술에 관해서 문제를 일으키기도 했습니다. 우리가 흔히 이용하는 지하철에서도 임베디드형으로 보이는 윈도우 XP가 계속해서 재부팅하고 있다거나 블루스크린 화면을 보여주는 경우를 간간히 경험할 수 있습니다. 얼마전에는 공영 주차장의 사전 정산기 시스템에 오류가 발생해서 재부팅을 요구하는 알림창을 보여주었으나 터치스크린의 먹통으로 인해 고객 입장에서 할 수 있는 방안은 없었습니다. 이처럼 개발자 입장에서 서비스를 만들어도 시스템을 실제로 이용하는 고객의 입장에서 지속적으로 서비스를 이용할 수 있는 가용성을 유지하는 것도 굉장히 중요한 부분이라고 생각됩니다.

## 서비스 장애
서비스 장애는 애플리케이션 코드에 의해서만 발생하지 않습니다. 요구사항을 반영하기 위해서 코드를 수정하고 추가한 개발자의 실수부터 시작으로 시스템을 구성하는 인프라에서도 예상하지 못한 문제가 발생합니다. 이와같이 서비스 장애는 정말 다양한 영역에서 발생하기 때문에 이러한 장애에 대한 경험이 없다면 쉽게 파악하기 힘들고 해결해나가는 과정이 어려울 수 있습니다. 저는 현재 조직에서 어떤 장애를 경험하였을까요?

1. AWS S3 SDK의 크레덴셜 만료
2. 쿼츠 스케줄러에 의한 스레드 무한 증식
3. KDB+ 시계열 데이터베이스 프로세스가 요청을 처리할 수 없음

> 사소한 문제를 포함한 수 많은 장애중에서 대표적인 이슈는 위와 같습니다.

### AWS S3 SDK 크레덴셜 만료
주 애플리케이션 서버와 별개로 사용자의 요청을 직접적으로 처리하지는 않지만 별도의 백그라운드 작업을 수행하는 모듈 서버에서 아마존 웹 서비스의 S3 SDK를 사용하는 코드에서 크레덴셜 만료로 인해 특정 스케줄 동작 시 오류가 발생하는 이슈가 확인되었습니다. EC2 인스턴스에 IAM Role을 지정하면 EC2 메타데이터에 의해 크레덴셜을 애플리케이션에서 사용할 수 있는데요. 기본적으로는 크레덴셜 프로바이더 체인에 의해서 여러가지 방식으로 크레덴셜을 가져오도록 코드를 작성합니다. 애플리케이션에서 사용하는 크레덴셜이 자동으로 갱신되는 옵션은 [SDK 이용 시 InstanceProfileCredentailsProvider 또는 체인을 생성자로 전달해야만 활성화](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html)된다는 정보가 있습니다.

> The automatic credentials refresh happens only when you use the default client constructor, which creates its own InstanceProfileCredentialsProvider as part of the default provider chain, or when you pass an InstanceProfileCredentialsProvider instance directly to the client constructor. If you use another method to obtain or pass instance profile credentials, you are responsible for checking for and refreshing expired credentials.

위 정보를 토대로 모듈 서버의 코드를 검수한 결과 공통 모듈에서 정의된 크레덴셜을 가져오는 유틸 클래스로부터 크레덴셜 프로바이더 체인을 가져와서 S3 인스턴스를 생성한 것이 아닌 크레덴셜을 프로바이더에 의해 발급하고 전달하여 사용했던 것을 확인하였습니다. 이는 리팩토링 과정에서 검수되지 않은 개발자의 명백한 실수이며 애플리케이션에서 크레덴셜이 자동으로 갱신되기 위해서는 InstanceProfileCredentialsProvider가 포함된 프로바이더 체인을 사용해야한다는 정보를 숙지하게 되었던 장애 경험이었습니다.

### 쿼츠 스케줄러에 의한 스레드 무한 증식
애플리케이션 배포 후 정상적으로 동작하던 애플리케이션이 사용자의 요청을 처리할 수 없는 상태가 되어버리는 문제를 확인하였습니다. 현재 조직은 모니터링 솔루션을 도입해서 사용해오던 조직은 아닙니다. 다만, 프로메테우스 및 그라파나 조합으로 모니터링 시스템을 구축하는 많은 글을 보고나서 개인적으로 사내 서버에 시스템을 구축해놓고 사용해왔는데요. 프로메테우스는 기본적으로 직접 HTTP 요청을 통해 지표를 수집하는 방식이므로 외부에서 접근이 불가능하도록 되어있는 애플리케이션의 지표를 수집하기 위해서 [Promethues Pushgateway](https://kdevkr.github.io/using-pushgateway-to-monitor-private-network-instance/)로 지표를 전달하도록 에이전트를 실행해놓고 모니터링을 시작했습니다. 해당 현상이 다시 발생한 시점에서 수집된 지표를 그라파나 대시보드를 통해 확인해본 결과 스레드의 수가 점점 증가하는 것을 보았고 스프링 부트 액추에이터의 스레드 덤프 엔드포인트를 통해 스레드를 분석해보니 SMS 발송 및 발송 상태 확인을 위한 스케줄에 의한 스레드가 수 없이 생성된 것을 확인했습니다. 해당 스케줄의 코드를 검토해본 결과 쿼츠 스케줄러 동작 방식을 고려하지 않고 스케줄 잡을 구현한 클래스에서 별도의 스레드를 생성하도록 코드가 작성되어 있는 것을 확인하게 되었습니다. 이 경험을 통해 쿼츠 스케줄러에 의해 스케줄이 동작할 때 이미 만들어져있는 빈 클래스를 사용하지 않고 스케줄러에 의해 동적으로 생성되어 처리된다는 점을 확인하게 되었습니다. 

### KDB+ 시계열 데이터베이스 프로세스가 요청을 처리할 수 없음
가장 크리티컬한 장애입니다. KDB+ 시계열 데이터베이스는 국내에서는 사용하지 않는 것처럼 보이는 상용 시계열 데이터베이스로써 KDB+ 프로세스가 요청을 처리할 수 없는 상태가 되나 프로세스가 종료되거나 별도로 프로세스에 의해 로그가 남는 것이 없으므로 이 문제에 대해 기술적인 도움을 요청하기에도 어려운 부분이었습니다. 프로세스가 사용자의 요청을 처리할 수 없는 상태가 되는 원인을 검토하기 위해서 여러가지 방법을 도입했습니다. 

1. 애플리케이션 슬로우 쿼리 로그
2. 서비스 상태 모니터링 솔루션

첫번째 방법은 애플리케이션으로 요청되는 KDB 쿼리 중 많은 시간이 소요되는 것들을 슬랙 채널로 메시지를 전송하는 것입니다. 그런데 수 많은 요청이 메시지 채널로 전달되는 문제가 확인되었습니다. 눈에는 보이지 않았지만 생각보다 많은 요청들이 밀리는 현상이 있었던 부분입니다. 두번째 방법은 Synthetic Monitoring라고 하는 서비스 상태 모니터링 솔루션을 도입하는 것이었습니다. 조직에서 일하지 않는 시간에도 서비스가 정상적으로 동작하는 상태인지를 확인하기 위한 방법으로 이전 프로메테우스를 통한 지표 모니터링과는 관점이 다릅니다. 

![](/images/posts/struggle-with-unknown-service-failures/uptime-kuma-01.png)

위는 서비스 상태 모니터링 도입 후 확인된 KDB+ 시계열 데이터베이스에 대한 연결 상태를 나타내는 지표입니다. 새벽 시간대에 응답 시간이 상당히 높아짐을 확인할 수 있었고 간간히 시계열 데이터베이스에 대한 연결 상태를 확인할 수 없게 됨을 확인할 수 있습니다. 특정 고객 환경에서 발생하는 현상이었으므로 해당 시간대에 어떤 요청을 수행하는 게 있는지 확인해보니 외부 업체에서 데이터를 수집하기 위해서 순간적으로 수 많은 API 요청을 하는 것을 알게 되었습니다. 해당 업체가 데이터를 조회할 때 많은 기간을 조회하면서 복수로 조건을 전달할 수 있음에도 불구하고 한 건씩 요청하다보니 서버에 많은 부하가 발생하게 된 부분입니다.

> 위와 같은 부분은 A예상하지 못한 API 부하이므로 작은 시스템 규모 상 디도스라고 자체 판단하여 API를 호출을 줄이도록 요청하였습니다.

![](/images/posts/struggle-with-unknown-service-failures/uptime-kuma-02.png)

API 요청 방식을 변경하고나서 응답 시간이 상당히 줄었으나 그대로 높다고 판단할 수 있는 지표입니다. 해당 업체가 사용하는 API에서 내부적으로 수행하는 KDB+ 쿼리를 검토해보니 다른 쿼리에 비해 상당히 성능이 느린점을 확인하였고 데이터 조회 및 불필요한 연산을 줄이는 방법을 채택하여 쿼리를 개선하고나서 현재는 다음과 같은 응답 시간을 보여주고 있습니다.

![](/images/posts/struggle-with-unknown-service-failures/uptime-kuma-03.png)

> 위의 결과를 보여주는 KDB+ 시계열 데이터베이스의 쿼리 최적화는 임시적인 대안일 뿐 프로세스가 기본적으로 사용자의 요청을 순차적으로 처리한다는 부분을 확인하게 되어 이를 개선할 수 있는 방안을 지속적으로 검토하고 있습니다.

이러한 장애에 대한 경험 공유는 현재 조직에는 이롭지 않은 부분일 수 있습니다. 그러나 개인적인 입장에서는 이러한 경험을 토대로 더 서비스를 성장할 수 있도록 만들어 갈 수 있다고 생각하는 바입니다. 많은 개발자가 자신이 경험한 문제와 어떻게 해결하였는지를 공유했으면 하는 바램입니다.

## 참고
- [우아~한 장애대응 - 우아한형제들 기술블로그](https://techblog.woowahan.com/4886/)
- [LINE의 장애 보고와 후속 절차 문화 - LINE Engineering](https://engineering.linecorp.com/ko/blog/line-failure-reporting-and-follow-up-process-culture/)
- [2022년 1월 100% 할인 이벤트 장애 부검 - 인프런](https://tech.inflab.com/202201-event-postmortem/)
